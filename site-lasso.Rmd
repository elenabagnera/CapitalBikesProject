---
title: "Lasso Regression"
author: ""
date: ""
output: html_document
---

```{r setup1, include = FALSE}
def.chunk.hook <- knitr::knit_hooks$get("chunk")
knitr::opts_chunk$set(cache = FALSE, warning = FALSE, message = FALSE)
knitr::knit_hooks$set(
  chunk = function(x, options) {
    x <- def.chunk.hook(x, options)
    ifelse(options$size != "normalsize", paste0("\n \\", options$size, "\n\n", x, "\n\n \\normalsize"), x)
  }
)
# knitr::knit_hooks$set(inline = function(x) {
#   prettyNum(round(x, 2), big.mark = ",")
# })
options(scipen = 999)
```

\begin{center}
\Huge{PPOL 670 | Final Project}

\Huge{Lasso Regression}
\end{center}

\vspace{0.1in}

[GitHub](https://github.com/elenabagnera/CapitalBikesProject)


```{r setup, include = FALSE}

library(tidyverse)
library(tidymodels)
library(recipes)
library(vip)
library(lubridate)
library(workflows)
library(ranger)

source("times.R")
source("io.R")
source("manipulate_data.R")
```

## Why LASSO?

LASSO regression is a variation of Linear Regression that uses Shrinkage. Shrinkage is a process that data values are shrunk towards a central point as the mean. This type of regression is well-suited for models showing heavy multi-collinearity (heavy correlation of features with each other).

Few points about Lasso Regression:
* It’s most often used for eliminating automated variables and the selection of features.   
* It’s well-suited for models showing heavy multi-collinearity (heavy correlation of features with each other).   
* LASSO regression utilizes L1 regularization   
* LASSO regression is considered to be better than Ridge as it selects only some features and decreases the coefficients of others to zero.
  
A tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term in ridge regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a central point, like the mean. Shrinkage results in simple, sparse models which are easier to analyze than high-dimensional data models with large numbers of parameters.
  
## Preparing the Data

```{r get_data, cache = FALSE, warning = FALSE, message = FALSE}

lagged_data <- read_csv('lagged_data_3200.csv') # or use 1600 if too much computationally?

model_data <- lagged_data %>%
  add_predictor_times() %>%
  filter(!is.na(departures) & !is.na(arrivals)) %>%
  select(-arrivals) %>% 
  format_weather()
```


```{r setup_data}
set.seed(28021995)

lasso_data <- lagged_data %>%
  add_predictor_times() %>%
  filter(!is.na(departures) & !is.na(arrivals)) %>%
  # Having real time arrival data would be cheating
  select(-arrivals) %>%
  format_weather()

# create a split object
lasso_data_split <- initial_split(lasso_data, prop = 0.8)

# create the training and testing data
lasso_train <- training(x = lasso_data_split)
lasso_test  <- testing(x = lasso_data_split)

# create 10-fold
folds <- vfold_cv(data = lasso_train, v = 10)
```


## Justification for Engeneering based on data exploration

From the exploratory data analysis phase, holidays clearly affected departures. When compared to the mean departures per day these holidays had significantly greater ridership compared to others which had much lower departures such as insert holidays.

To help improve our predictions, we add step_holiday to our recipe to control for outlier trends during holidays. Additionally, as shown in the exploratory phase, the distribution of our outcome variable, Lincoln Memorial Departures is skewed. To avoid this imbalance affecting our predictions, step_BoxCox works since departures is a strictly positive variable but it does have 0s (hence we cannot use logs). We use it to transform the outcome variable so that it would be more like a normal distribution.

Besides, we add `step_dummy` and `step_center` in recipe. `step_dummy()` creates a specification of a recipe step that will convert nominal data (e.g. character or factors) into one or more numeric binary model terms for the levels of the original data. `step_center()` creates a specification of a recipe step that will normalize numeric data to have a mean of zero.


## Model Implementation

```{r recipe}

# create a recipe
lasso_recipe <-
  recipe(formula = departures ~ ., data = lasso_train) %>%
  step_holiday(date, holidays = timeDate::listHolidays("US"), keep_original_cols = FALSE) %>% 
  step_nzv(all_predictors()) %>% # 
  step_other(all_nominal_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>%
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_BoxCox(all_outcomes()) # this is because we have 0s and cannot use log, did not work
```

```{r run the model}

# create a model
lasso_mod <- linear_reg(
  penalty = tune(), 
  mixture = 1) %>%
  set_engine("glmnet")

# create a workflow
lasso_workflow  <- 
  workflow() %>% 
  add_model(spec = lasso_mod) %>% 
  add_recipe(recipe = lasso_recipe)

# create a tuning grid
lasso_grid <- grid_regular(penalty(), levels = 10)

# estimate with resampling
lasso_res <- 
  lasso_workflow %>% 
  tune_grid(resample = folds,
            grid = lasso_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))
```


## Selecting the Best Model and Fitting

```{r select}

# select best model
lasso_best <- lasso_res %>% 
            select_best("rmse")

### create a new model ###
lasso_final <- finalize_workflow(
  lasso_workflow,
  parameters = lasso_best
)

# fit to the training data and extract coefficients
lasso_coefs <- lasso_final %>%
  fit(data = lasso_train) %>%
  extract_fit_parsnip() %>%
  vi(lambda = lasso_best$penalty)

# fit resample
lasso_fit_rs <-
  lasso_final %>% 
  fit_resamples(resample = folds)
```

## Results

```{r variable importance}
library(ggplot2)
# plot result of variable importance
lasso_coefs %>% 
  slice_max(Importance, n = 15) %>% 
  mutate(Variable = forcats::fct_reorder(.x = Importance,
                                      .f = Variable)) %>% 
  ggplot(aes(y = Importance, x = Variable)) +
  geom_col(width = 0.7, fill = "lightskyblue3", alpha = 0.8) +
  coord_flip() +
  theme_minimal() + 
  labs(title = "Sunlights has the highest coefficient in predicting ridership") +
  theme(plot.title = element_text(face = "bold", size = 12, hjust = 1))
```

```{r results, include = FALSE}

# collect metrics
plot <- collect_metrics(lasso_fit_rs, summarize = FALSE)

# plot rmse
LASSO <- plot %>% 
  filter(.metric == "rmse") %>% 
  ggplot(aes(id, .estimate, group = .estimator)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(limits = c(0, 8)) +
  labs(title = "Calculated RMSE Across the 10 Folds - Lasso Regression",
       y = "RMSE_hat") +
  theme_minimal()
```

## Discussion of Results

The LASSO regression produced an average RMSE of 4. This is a relatively big RMSE compared to what we aim for (range 1-2). This could be due to several weaknesses pertaining to LASSO. With the RMSE around 4, it is likely that we have a model that badly overfits the data. We will elaborate in details in discussion.

We also run into the issue of not being able to run Box-Cox as there are non-positive values in selected variable.
