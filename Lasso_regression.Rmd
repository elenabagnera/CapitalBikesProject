---
title: "Lasso_regression"
author: ""
date: ""
output: html_document
---

```{r include = FALSE}
def.chunk.hook <- knitr::knit_hooks$get("chunk")
knitr::opts_chunk$set(cache = FALSE)
knitr::knit_hooks$set(
  chunk = function(x, options) {
    x <- def.chunk.hook(x, options)
    ifelse(options$size != "normalsize", paste0("\n \\", options$size, "\n\n", x, "\n\n \\normalsize"), x)
  }
)
# knitr::knit_hooks$set(inline = function(x) {
#   prettyNum(round(x, 2), big.mark = ",")
# })
options(scipen = 999)
```

\begin{center}
\Huge{PPOL 670 | Final Project}

\Huge{Lasso Regression}
\end{center}

\vspace{0.1in}

[GitHub](https://github.com/elenabagnera/CapitalBikesProject)

## Setup

```{r setup, message = FALSE, warnings = FALSE}

library(tidyverse)
library(tidymodels)
library(recipes)
library(vip)
library(lubridate)
library(workflows)
library(ranger)

source("times.R")
source("io.R")
source("manipulate_data.R")
```

## Why LASSO

LASSO regression is a variation of Linear Regression that uses Shrinkage. Shrinkage is a process that data values are shrunk towards a central point as the mean. This type of regression is well-suited for models showing heavy multicollinearity (heavy correlation of features with each other).

Few points about Lasso Regression:
  •	It’s most often used for eliminating automated variables and the selection of features.
  •	It’s well-suited for models showing heavy multicollinearity (heavy correlation of features with each other).
  •	LASSO regression utilizes L1 regularization
  •	LASSO regression is considered to be better than Ridge as it selects only some features and decreases the coefficients of others to zero.
  
ADD EXPLANATION OF TUNING
  
## Preparing the data

```{r get_data, cache = FALSE, warning = FALSE, message = FALSE}

lagged_data <- read_csv('lagged_data_3200.csv') # or use 1600 if too much computationally?

model_data <- lagged_data %>%
  add_predictor_times() %>%
  filter(!is.na(departures) & !is.na(arrivals)) %>%
  # Having real time arrival data would be cheating
  select(-arrivals) %>% 
  format_weather()
```


```{r setup_data}

# to run the three models, we only need this chunk once?

set.seed(28021995)

lasso_data <- lagged_data %>%
  add_predictor_times() %>%
  filter(!is.na(departures) & !is.na(arrivals)) %>%
  # Having real time arrival data would be cheating
  select(-arrivals) %>%
  format_weather()

# create a split object
lasso_data_split <- initial_split(lasso_data, prop = 0.8)

# create the training and testing data
lasso_train <- training(x = lasso_data_split)
lasso_test  <- testing(x = lasso_data_split)

# create 10-fold
folds <- vfold_cv(data = lasso_train, v = 10)
```

## Justification for Engeneering based on data exploration

(take from decision tree explanation but add something on step scale step dummy and step center)


## Model implementation

```{r recipe}

# create a recipe
lasso_recipe <-
  recipe(formula = departures ~ ., data = lasso_train) %>%
  step_holiday(date, holidays = timeDate::listHolidays("US"), keep_original_cols = FALSE) %>% # Aaron said we should put more weight on some holidays rather than other, for example 4th of July and inauguration day especially since we are looking at Lincoln memorial
  step_nzv(all_predictors()) %>% # Aaron suggested adding this
  step_other(all_nominal_predictors()) %>% # add this for any categorical predictors
  step_dummy(all_nominal_predictors()) %>%
  step_center(all_numeric_predictors()) %>% 
  step_scale(all_numeric_predictors()) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_BoxCox(all_outcomes()) # this is because we have 0s and cannot use log 

# see the outcome data
bake(prep(lasso_recipe, training = lasso_train), new_data = lasso_test)
```

```{r run the model}

# create a model
lasso_mod <- linear_reg(
  penalty = tune(), 
  mixture = 1) %>%
  set_engine("glmnet")

# create a workflow
lasso_workflow  <- 
  workflow() %>% 
  add_model(spec = lasso_mod) %>% 
  add_recipe(recipe = lasso_recipe)

# create a tuning grid
lasso_grid <- grid_regular(penalty(), levels = 10)

# estimate with resampling
lasso_res <- 
  lasso_workflow %>% 
  tune_grid(resample = folds,
            grid = lasso_grid,
            control = control_grid(save_pred = TRUE),
            metrics = metric_set(rmse))


```


## Selecting the best model and fitting

```{r select}

# select best model
lasso_best <- lasso_res %>% 
            select_best("rmse")

### create a new model ###
lasso_final <- finalize_workflow(
  lasso_workflow,
  parameters = lasso_best
)

# fit to the training data and extract coefficients
lasso_coefs <- lasso_final %>%
  fit(data = lasso_train) %>%
  extract_fit_parsnip() %>%
  vi(lambda = lasso_best$penalty) 

lasso_coefs

# fit resample
lasso_fit_rs <-
  lasso_final %>% 
  fit_resamples(resample = folds)

```


## Results

```{r results}

# collect metrics
plot <- collect_metrics(lasso_fit_rs, summarize = FALSE)

# plot rmse
plot %>% 
  filter(.metric == "rmse") %>% 
  ggplot(aes(id, .estimate, group = .estimator)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(limits = c(0, 8)) +
  labs(title = "Calculated RMSE Across the 10 Folds - Lasso Regression",
       y = "RMSE_hat") +
  theme_minimal()
```

## Discussion of results

ADD TWO LINES EXPLAINING WHAT RESULTS WE GOT similar to decision tree (copied below)

The decision tree estimation produced an average RMSE of5.  This is a relatively big RMSE compared to what we wanted (range 2-3). 

This could be due to several weaknesses pertaining to decision trees as they tend to overfit which means that it will be difficult to generalize to novel data. They also tend to produce trees that are overly correlated with each other (as it is more likely to get swayed by points that split the attributes well in both our predictor variables and outcome variable). We could expect random forest to perform better. 

