---
title: "Model Comparison"
author: "Farah Kaddah"
date: "12/14/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Comparing RMSEs of our models 

*This model* does better than all other models in terms of RMSE.


## Now let's run our model on the testing data to see how it perform?

*this is the result which is expected to have a higher RMSE*

## Decision Tree vs Random Forests

Having a group of decision tree creates “randomness”  and avoids common weaknesses in predictions based on one decision tree like over fitting and excessive correlation between variables. This is because: 
      1. random forests algorithm uses a subset of features as candidates at each split (does not rely          on the same set of features:: de- correlates individual trees) 
      2. Each tree uses a random sample from the training dataset as it generates the splits which             inserts an aspect of randomness and prevents overfitting
      
Overall, Random Forests should produce more accurate  predictions than one decision tree. 


## Random Forests vs LASSO

Random forests will do better in predictions of hourly departures in  the Lincoln Memorial Station whereas LASSO will do better in feature/variable selection (identifying which variables predict our outcome best). 

Decision Trees are great for obtaining non-linear relationships between input features and the target variable.  However, they pose a major challenge that is that they can’t extrapolate outside unseen data which is something that LASSO models are able to compute. 
