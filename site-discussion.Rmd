---
title: "Discussion & Implementing the final model"
author: ""
date: ""
output: html_document
---

```{r include = FALSE}
def.chunk.hook <- knitr::knit_hooks$get("chunk")
knitr::opts_chunk$set(cache = FALSE)
knitr::knit_hooks$set(
  chunk = function(x, options) {
    x <- def.chunk.hook(x, options)
    ifelse(options$size != "normalsize", paste0("\n \\", options$size, "\n\n", x, "\n\n \\normalsize"), x)
  }
)
# knitr::knit_hooks$set(inline = function(x) {
#   prettyNum(round(x, 2), big.mark = ",")
# })
options(scipen=999)
```


\begin{center}
\Huge{PPOL 670 | Final Project}

\Huge{Example}
\end{center}

\vspace{0.1in}

## Comparing RMSEs of our models [ADD PLOT]

```{r comparison}

LASSO + DT + RF

# the models are comparable for prediction accuracy
# bind_rows(
#   `lm` = show_best(lm_cv, metric = "rmse", n = 1),
#   `LASSO` = show_best(lasso_cv, metric = "rmse", n = 1),
#   `ridge` = show_best(ridge_fit, metric = "rmse",n = 1),
#   `enet` = show_best(elastic_net_fit, metric = "rmse", n = 1),
#   .id = "model"
# )
# 
# all_coefs <- bind_rows(
#   `lm` = lm_coefs,
#   `LASSO` = lasso_coefs,
#   `ridge` = ridge_coefs,
#   `enet` = elastic_net_coefs,
#   .id = "model"
# ) 
# 
# all_coefs %>%
#   group_by(model) %>%
#   slice_max(Importance, n = 10) %>%
#   ggplot(aes(Importance, Variable, fill = model)) +
#   geom_col(position = "dodge")
# 
# all_coefs %>%
#   filter(model != "lm") %>%
#   group_by(model) %>%
#   slice_max(Importance, n = 10) %>%
#   ggplot(aes(Importance, Variable, fill = model)) +
#   geom_col(position = "dodge")
# 
# 
# # compare the regularized coefficients to the lm coefficients for all three models
# plot1 <- left_join(
#   rename(lm_coefs, lm = Importance),
#   rename(lasso_coefs, LASSO = Importance),
#   by = "Variable"
# ) %>%
#   ggplot(aes(lm, LASSO)) +
#   geom_point(alpha = 0.3) +
#   scale_y_continuous(limits = c(0, 0.08))
# 
# plot2 <- left_join(
#   rename(lm_coefs, lm = Importance),
#   rename(ridge_coefs, ridge = Importance),
#   by = "Variable"
# ) %>%
#   ggplot(aes(lm, ridge)) +
#   geom_point(alpha = 0.3) +
#   scale_y_continuous(limits = c(0, 0.08))
# 
# plot3 <- left_join(
#   rename(lm_coefs, lm = Importance),
#   rename(elastic_net_coefs, enet = Importance),
#   by = "Variable"
# ) %>%
#   ggplot(aes(lm, enet)) +
#   geom_point(alpha = 0.3) +
#   scale_y_continuous(limits = c(0, 0.08))
# 
# plot1 + plot2 + plot3

``` 


As seen from the *graph/table*, Random forests does better than decision tree and LASSO models in terms of RMSE. 
- Decision tree lowest RMSE: 4.25
- Random Forests  lowest RMSE: 3.95
- LASSO lowest RMSE: 4

Some potential reasons for Random Forests out-performing decision trees and LASSO: 

## Having a group of decision tree creates “randomness”  and avoids common weaknesses in predictions based on one decision tree like over fitting and excessive correlation between variables. This is because: 
      1. random forests algorithm uses a subset of features as candidates at each split (does not rely on         the same set of features:: de- correlates individual trees) 
      2. Each tree uses a random sample from the training dataset as it generates the splits which inserts        an aspect of randomness and prevents overfitting
      
## Compared to LASSO regression, Random forests decision trees are great for obtaining non-linear relationships between input features and the target variable.  
It also operates within sample, as opposed to LASSO models which could potentially provide out of sample predictions. In our case, this might be risky because we can not have negative departures, and since departures could go to very high levels in general, we should be careful in analyzing predictions that give very high hourly departure estimation which could be unrealistic. Although we expected LASSO to do a better job in avoiding multicollinearity between our predictors, it is difficult to judge for sure. 


```{r setup_data}

set.seed(28021995)

# create a split object
data_split <- initial_split(model_data, prop = 0.8)

# create the training and testing data
train <- training(x = data_split)
test <- testing(x = data_split)

```


##Implementing Best model

Now let's run our model on the full training data to see how it performs

```{r get_data, cache=FALSE, warning=FALSE, message=FALSE}

final_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(final_rf) %>%
  fit(train)

#  cbs_data <- read_files('data/')
# data <- sep_departures_from_arrivals(cbs_data) %>%
#   filter(!is.na(station)) # Remove electric bikes not going to / from a station
# 
# data <- sep_departures_from_arrivals(cbs_data) %>%
#   filter(!is.na(station))
# 
# filtered_data <- filter_by_distance(data, from_station = 'lincoln_memorial', distance_m = 400)
# 
# hour_data <- get_station_hourly(filtered_data) %>%
#   get_historic_weather() %>% 
#   add_sun_is_out() 
#   #filter(conditions != "Bad") # To test if bad weather really hurts RMSE that much
# 
# lagged_data <- setup_for_time_prediction(hour_data, station_name = "lincoln_memorial")
# 
# model_data <- lagged_data %>%
#   add_predictor_times() %>%
#   filter(!is.na(departures) & !is.na(arrivals)) %>%
#   # Having real time arrival data would be cheating
#   select(-arrivals) %>%
#   format_weather()

```


## Generating predictions on the testing data
Now let's run our final model on the testing data, which we had put to the side

```{r testing}

p_test <- bind_cols(
  test,
  predict(object = final_wf, new_data = test))

#Out of sample RMSE
sqrt(mean((p_test$departures - p_test$.pred)^2))


# predictions <- bind_cols(
# test,
# predict(object = best_fit, new_data = test), 
# ) %>%
#   select(departures, starts_with(".pred"))
# 
# predictions %>%
# rmse(departures, .pred) 

``` 


## What are the most important predictors of hourly departures at the Lincoln Memorial Station?

```{r try}

# model_fit %>%
#   pull_workflow_fit() %>%
#   vip(num_features = 10) ## variable importance
  
final_rf %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(departures ~ .,
    data = train
  ) %>%
  vip(geom = "point")
```

```
Based on the variable impoertance plot, our most important predictors are:
 
 1. Hour: as expected, the hour has a big influence on predicting departures at Lincoln Memorial Station. We know from the exploration period that there is some hours *(1-6 am)* where deparutres are nearly non exitent, compared to peaking in the evening around *(4-6 pm)*
 
 2. Sun: 
 Our results also suggest that departures are associated with sunlight. This is reasonable given that people are more likely to ride bikes duirng daylight 
 
 3. Year day
 ******Year days ** not sure
 
 4. Week day 
From earlier data exploration we could see that weekdays have a strong rleationship with departures (less dpeartures during weekends). This is a reasonable prediction.
 
 5. 4th Street and Maddison DR. Station departures and arrivals
This suggests that a lot trips go back and forth between Lincoln Memorial Station and  4th Street and Maddison DR. Station
 

## Ideas for future projects

* Try xgboost
* Account for the pandemic somehow (for example over the summer 2020 when there was a curfew)
* Try a version where we first implement a random forest model to predict a binary variable of whether bikes are departing or not. Then use the predictions to create a new variable and estimate a second random forest model that uses that variable as a predictor
* Instead of using RMSE, we could use a range of error that we envision for our project (eg. we can aim at always haviing 3 bikes at a station-- and hence our prediction would essentially be wether or not we have 3 bikes!)

