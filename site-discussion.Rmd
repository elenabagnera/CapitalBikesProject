---
title: "Discussion & Implementing the final model"
author: ""
date: ""
output: html_document
---

```{r include = FALSE}
def.chunk.hook <- knitr::knit_hooks$get("chunk")
knitr::opts_chunk$set(cache = FALSE)
knitr::knit_hooks$set(
  chunk = function(x, options) {
    x <- def.chunk.hook(x, options)
    ifelse(options$size != "normalsize", paste0("\n \\", options$size, "\n\n", x, "\n\n \\normalsize"), x)
  }
)
# knitr::knit_hooks$set(inline = function(x) {
#   prettyNum(round(x, 2), big.mark = ",")
# })
options(scipen=999)
```


\begin{center}
\Huge{PPOL 670 | Final Project}

\Huge{Example}
\end{center}

\vspace{0.1in}

## Comparing RMSEs of our models [ADD PLOT]

```{r comparison}
## from class notes 
# the models are comparable for prediction accuracy
bind_rows(
  `lm` = show_best(lm_cv, metric = "rmse", n = 1),
  `LASSO` = show_best(lasso_cv, metric = "rmse", n = 1),
  `ridge` = show_best(ridge_fit, metric = "rmse",n = 1),
  `enet` = show_best(elastic_net_fit, metric = "rmse", n = 1),
  .id = "model"
)

all_coefs <- bind_rows(
  `lm` = lm_coefs,
  `LASSO` = lasso_coefs,
  `ridge` = ridge_coefs,
  `enet` = elastic_net_coefs,
  .id = "model"
) 

all_coefs %>%
  group_by(model) %>%
  slice_max(Importance, n = 10) %>%
  ggplot(aes(Importance, Variable, fill = model)) +
  geom_col(position = "dodge")

all_coefs %>%
  filter(model != "lm") %>%
  group_by(model) %>%
  slice_max(Importance, n = 10) %>%
  ggplot(aes(Importance, Variable, fill = model)) +
  geom_col(position = "dodge")


# compare the regularized coefficients to the lm coefficients for all three models
plot1 <- left_join(
  rename(lm_coefs, lm = Importance),
  rename(lasso_coefs, LASSO = Importance),
  by = "Variable"
) %>%
  ggplot(aes(lm, LASSO)) +
  geom_point(alpha = 0.3) +
  scale_y_continuous(limits = c(0, 0.08))

plot2 <- left_join(
  rename(lm_coefs, lm = Importance),
  rename(ridge_coefs, ridge = Importance),
  by = "Variable"
) %>%
  ggplot(aes(lm, ridge)) +
  geom_point(alpha = 0.3) +
  scale_y_continuous(limits = c(0, 0.08))

plot3 <- left_join(
  rename(lm_coefs, lm = Importance),
  rename(elastic_net_coefs, enet = Importance),
  by = "Variable"
) %>%
  ggplot(aes(lm, enet)) +
  geom_point(alpha = 0.3) +
  scale_y_continuous(limits = c(0, 0.08))

plot1 + plot2 + plot3

``` 


*This model* does better than all other models in terms of RMSE.

*since it is more likely to be the random forest here is a justification to why it is doing better than a decision tree*

*Note to be used if required: Note: A Random Forest regressor may or may not perform better than the Decision Tree in regression (while it usually performs better in classification), because of the delicate overfitting-underfitting tradeoff in the nature of tree-constructing algorithms.*

## Having a group of decision tree creates “randomness”  and avoids common weaknesses in predictions based on one decision tree like over fitting and excessive correlation between variables. This is because: 
      1. random forests algorithm uses a subset of features as candidates at each split (does not rely on the same set of features:: de- correlates individual trees) 
      2. Each tree uses a random sample from the training dataset as it generates the splits which inserts an aspect of randomness and prevents overfitting
      
Overall, Random Forests should produce more accurate predictions than one decision tree.


##Implementing Best model

Now let's run our model on the full training data to see how it performs

```{r get_data, cache=FALSE, warning=FALSE, message=FALSE}

 cbs_data <- read_files('data/')
data <- sep_departures_from_arrivals(cbs_data) %>%
  filter(!is.na(station)) # Remove electric bikes not going to / from a station

data <- sep_departures_from_arrivals(cbs_data) %>%
  filter(!is.na(station))

filtered_data <- filter_by_distance(data, from_station = 'lincoln_memorial', distance_m = 400)

hour_data <- get_station_hourly(filtered_data) %>%
  get_historic_weather() %>% 
  add_sun_is_out() 
  #filter(conditions != "Bad") # To test if bad weather really hurts RMSE that much

lagged_data <- setup_for_time_prediction(hour_data, station_name = "lincoln_memorial")

model_data <- lagged_data %>%
  add_predictor_times() %>%
  filter(!is.na(departures) & !is.na(arrivals)) %>%
  # Having real time arrival data would be cheating
  select(-arrivals) %>%
  format_weather()

```



```{r setup_data}

set.seed(28021995)

# create a split object
data_split <- initial_split(model_data, prop = 0.8)

# create the training and testing data
train <- training(x = data_split)
test <- testing(x = data_split)

```

```{r fulltraining}

# basically here run the best out of our models but on the train dataset without folds




```   


## Generating predictions on the testing data

Now let's run our final model on the testing data, which we had put to the side

```{r testing}

predictions <- bind_cols(
test,
predict(object = best_fit, new_data = test), 
) %>%
  select(departures, starts_with(".pred"))

predictions %>%
rmse(departures, .pred) 

``` 


## Understanding variable importance


```{r try}

model_fit %>%
  pull_workflow_fit() %>%
  vip(num_features = 10) ## variable importance

```

*this is the result which is expected to have a higher RMSE*

## What are the most important predictors of hourly departures at the Lincoln Memorial Station?

We expect LASSO do better on this aspect. While random forests will do better in predictions of hourly departures in  the Lincoln Memorial Station *(as evidenced by lower RMSE)*, LASSO will do better in feature/variable selection (identifying which variables predict our outcome best). 


 Our most important predictors are *insert predictors* -- *show a graph here possibly*

*Note to be used if required: Decision Trees are great for obtaining non-linear relationships between input features and the target variable.  However, they pose a major challenge that is that they can’t extrapolate outside unseen data which is something that LASSO models are able to compute. *

## Ideas for future projects

* Try xgboost
* Account for the pandemic somehow (for example over the summer 2020 when there was a curfew)
* Try a version where we first implement a random forest model to predict a binary variable of whether bikes are departing or not. Then use the predictions to create a new variable and estimate a second random forest model that uses that variable as a predictor

