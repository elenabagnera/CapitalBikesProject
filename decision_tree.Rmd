---
title: "Decision tree"
author: "Elena Bagnera"
date: "12/12/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r include = FALSE}
def.chunk.hook <- knitr::knit_hooks$get("chunk")
knitr::opts_chunk$set(cache = FALSE)
knitr::knit_hooks$set(
  chunk = function(x, options) {
    x <- def.chunk.hook(x, options)
    ifelse(options$size != "normalsize", paste0("\n \\", options$size, "\n\n", x, "\n\n \\normalsize"), x)
  }
)
# knitr::knit_hooks$set(inline = function(x) {
#   prettyNum(round(x, 2), big.mark = ",")
# })
options(scipen=999)
```

\begin{center}
\Huge{PPOL 670 | Final Project}

\Huge{Random Forest}
\end{center}

\vspace{0.1in}

[GitHub](https://github.com/elenabagnera/CapitalBikesProject)

## Setup

```{r setup, message=FALSE include=FALSE}
library(tidyverse)
library(tidymodels)
library(recipes)
library(vip)
library(lubridate)
library(workflows)
library(ranger)
source("times.R")
source("io.R")
source("manipulate_data.R")
doParallel::registerDoParallel()
library(rpart.plot)  
library(vip)  
library(rpart)
library(caret)
```

## Why Decision Trees

Tree-based models are a class of nonparametric algorithms that work by partitioning the feature space into a number of smaller (non-overlapping) regions with similar response values using a set of splitting rules. In essence, our tree is a set of rules that allows us to make predictions by asking simple yes-or-no questions about each feature. It's a good place to start because it's a simple algorithm and it will tell us whether it's worth doing a random forest.

## Preparing the data

```{r get_data, cache=FALSE, warning=FALSE, message=FALSE}

 cbs_data <- read_files('data/')
data <- sep_departures_from_arrivals(cbs_data) %>%
  filter(!is.na(station)) # Remove electric bikes not going to / from a station

data <- sep_departures_from_arrivals(cbs_data) %>%
  filter(!is.na(station))

filtered_data <- filter_by_distance(data, from_station = 'lincoln_memorial', distance_m = 400)

hour_data <- get_station_hourly(filtered_data) %>%
  get_historic_weather() %>% 
  add_sun_is_out() 
  #filter(conditions != "Bad") # To test if bad weather really hurts RMSE that much

lagged_data <- setup_for_time_prediction(hour_data, station_name = "lincoln_memorial")

model_data <- lagged_data %>%
  add_predictor_times() %>%
  filter(!is.na(departures) & !is.na(arrivals)) %>%
  # Having real time arrival data would be cheating
  select(-arrivals) %>%
  format_weather()

```



```{r setup_data}

set.seed(28021995)

# create a split object
data_split <- initial_split(model_data, prop = 0.8)

# create the training and testing data
train <- training(x = data_split)
test <- testing(x = data_split)

folds <- vfold_cv(data = train, v = 10)


```


## Justification for Engineering based on data exploration

From the exploratory data analysis phase, holidays clearly affected departures. When compared to the mean departures per day *these holidays* had significantly greater ridership compared to others which had much lower departures such as *insert holdays* To help improve our predictions, we add step_holiday to our recipe to control for outlier trends during holidays. Additionally, as shown in the exploratory phase, the distribution of our outcome variable, `Lincoln Memorial Departures` is skewed. To avoid this imbalance affecting our predictions, step_BoxCox works since departures is a strictly positive variable but it does have 0s (hence we cannot use logs). We use it to transform the outcome variable so that it would be more like a normal distribution.

step_other further improves our prediction as it removes any infrequently reoccurring observations  in both our predictors and outcome variable. Finally, we used the near zero variance filter: step_nzv, to account for any predictors in our models that donâ€™t have variance (one or very few unique values) and where these few unique values are far away from each other.  

## Model Implementation
  
```{r dec}

# creating a recipe
recipe <-
  recipe(formula = departures ~ ., data = train) %>%
  step_holiday(date, holidays = timeDate::listHolidays("US"), keep_original_cols = FALSE) %>% 
  step_nzv(all_predictors()) %>% 
  step_BoxCox(all_outcomes()) 

# create a model

dec_mod <- decision_tree() %>%
  set_engine(engine="rpart") %>%
  set_mode(mode="regression")

# create a workflow

workflow_dec <-
  workflow() %>%
  add_model(spec = dec_mod) %>%
  add_recipe(recipe = recipe)


#fit the model on training data 

model_fit <-
workflow_dec %>%
fit_resamples(resamples = folds)

```

We can also look at variable importance:

```{r try}

model_fit %>%
  pull_workflow_fit() %>%
  vip(num_features = 10) ## variable importance

```

## Evaluation

```{r dec_rmse}

collect_metrics(model_fit) 

collect_metrics(model_fit, summarize = FALSE) %>%
  filter(.metric == "rmse") %>%
  ggplot(aes(id, .estimate, group = .estimator)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(limits = c(0, 10)) +
  labs(title = "Calculated RMSE Across the 10 Folds",
       y = "RMSE_hat") +
  theme_minimal() 

```

## Discussion of results

The decision tree estimation produced an average RMSE of5.  This is a relatively big RMSE compared to what we wanted (range 2-3). 

This could be due to several weaknesses pertaining to decision trees as they tend to overfit which means that it will be difficult to generalize to novel data. They also tend to produce trees that are overly correlated with each other (as it is more likely to get swayed by points that split the attributes well in both our predictor variables and outcome variable). We could expect random forest to perform better. 

