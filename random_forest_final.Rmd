---
title: "Random Forest Final Model"
author: ""
date: ""
output: html_document
---

```{r setup1, include = FALSE}
def.chunk.hook <- knitr::knit_hooks$get("chunk")
knitr::opts_chunk$set(cache = FALSE)
knitr::knit_hooks$set(
  chunk = function(x, options) {
    x <- def.chunk.hook(x, options)
    ifelse(options$size != "normalsize", paste0("\n \\", options$size, "\n\n", x, "\n\n \\normalsize"), x)
  }
)
# knitr::knit_hooks$set(inline = function(x) {
#   prettyNum(round(x, 2), big.mark = ",")
# })
options(scipen=999)
```

\begin{center}
\Huge{PPOL 670 | Final Project}

\Huge{Random Forest}
\end{center}

\vspace{0.1in}

[GitHub](https://github.com/elenabagnera/CapitalBikesProject)

```{r setup, include=FALSE}
library(tidyverse)
library(tidymodels)
library(recipes)
library(vip)
library(lubridate)
library(workflows)
library(ranger)
source("times.R")
source("io.R")
source("manipulate_data.R")
doParallel::registerDoParallel()
```

## Comparing RMSEs of our models [ADD PLOT]

As seen from the *graph/table*, Random forests does better than decision tree and LASSO models in terms of RMSE. 
- Decision tree lowest RMSE: 4.25
- Random Forests  lowest RMSE: 3.95
- LASSO lowest RMSE: 4

Some potential reasons for Random Forests out-performing decision trees and LASSO: 

## Having a group of decision tree creates “randomness”  and avoids common weaknesses in predictions based on one decision tree like over fitting and excessive correlation between variables. This is because: 
      1. random forests algorithm uses a subset of features as candidates at each split (does not rely on         the same set of features:: de- correlates individual trees) 
      2. Each tree uses a random sample from the training dataset as it generates the splits which inserts        an aspect of randomness and prevents overfitting
      
## Compared to LASSO regression, Random forests decision trees are great for obtaining non-linear relationships between input features and the target variable.  
It also operates within sample, as opposed to LASSO models which could potentially provide out of sample predictions. In our case, this might be risky because we can not have negative departures, and since departures could go to very high levels in general, we should be careful in analyzing predictions that give very high hourly departure estimation which could be unrealistic. Although we expected LASSO to do a better job in avoiding multicollinearity between our predictors, it is difficult to judge for sure. 


## Preparing the Data

```{r get_data, cache=TRUE, warning=FALSE, message=FALSE}

lagged_data <- read_csv('lagged_data_3200.csv')
hour_data <- read_csv('hour_data_3200.csv')

model_data <- lagged_data %>%
  add_predictor_times() %>%
  filter(!is.na(departures) & !is.na(arrivals)) %>%
  # Having real time arrival data would be cheating
  select(-arrivals) %>% 
  format_weather()
```


```{r setup_data}

set.seed(28021995)

# create a split object
data_split <- initial_split(model_data, prop = 0.8)

# create the training and testing data
train <- training(x = data_split)
test  <- testing(x = data_split)
```

## Our preprocessing for Random Forests is the same as decision trees because Random Forests is just a "more complex" decision tree.

```{r recipe, cache=TRUE}
recipe <-
  recipe(formula = departures ~ ., data = train) %>%
  step_holiday(date, holidays = timeDate::listHolidays("US"), keep_original_cols = FALSE) %>% 
  step_nzv(all_predictors()) 
  # step_BoxCox(all_outcomes()) # Does not run with zero in the values
```

## Model Implementation

```{r tuning, cache=TRUE}
# Tuning min_n doesn't seem to help
mod <- rand_forest(mtry = 250, trees = 1000) %>%
  set_mode("regression") %>%
  set_engine("ranger")

wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(mod)
```


```{r chosen_one, cache=TRUE}
final_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(mod) %>%
  fit(train)

p_test <- bind_cols(
  test,
  predict(object = final_wf, new_data = test))
```

## Results

Out of sample RMSE

```{r OOS_RMSE_lincoln}
sqrt(mean((p_test$departures - p_test$.pred)^2))
write_csv(p_test, "final-p-test.csv")
```

```{r lincoln_final_vip, warning=FALSE, eval=FALSE}
mod %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(departures ~ .,
    data = train
  ) %>%
  vip(geom = "point")
```

Based on the variable importance plot, our most important predictors are:
 
 1. Hour: as expected, the hour has a big influence on predicting departures at Lincoln Memorial Station. We know from the exploration period that there is some hours *(1-6 am)* where departures are nearly non existance, compared to peaking in the evening around *(2 pm)*
 
 2. Sun: 
 Our results also suggest that departures are associated with sunlight. This is reasonable given that people are more likely to ride bikes duirng daylight 
 
 3. Year day
 It is likely that year day affects departures because it reflects seasons which has many implications, including but not limited to weather changes due to seasons, but also other periods like the summer holidays.

 4. Week day 
From earlier data exploration we could see that weekdays have a strong relationship with departures (less dpeartures during weekends). This is a reasonable prediction.
 
 5. 4th Street and Maddison DR. Station departures and arrivals
This suggests that a lot trips go back and forth between Lincoln Memorial Station and  4th Street and Maddison DR. Station. Trips back and forth from near the Smithonian to Lincoln Memorial seem popular among cpaital bikeshare riders. 

## Discussion of Results

### Average departure per hour

```{r mean_of_departures}
p_test %>% summarize(mean(departures))
```

### Top 20 hours of 12431 hours.

```{r top_20_departures}
p_test %>% arrange(-departures) %>% select(departures) %>% head(20)
```

### Top 20 Error - very high departure days

```{r top_20_error}
p_test %>% 
  mutate(error = abs(.pred - departures)) %>% 
  select(year, month, day, hour, weekday, departures, .pred, error, conditions,temperature, cloud_cover, snow, precipitation, lincoln_memorial_departures) %>% 
  arrange(-error) %>% 
  head(20)
```

### Best guesses

```{r}
p_test %>% 
  mutate(error = abs(.pred - departures)) %>% 
  select(year, month, day, hour, weekday, departures, .pred, error, conditions,temperature, cloud_cover, snow, precipitation, lincoln_memorial_departures)  %>% 
  arrange(error) %>% 
  head(15)
```

### Best guesses with more 5 departures

```{r}
p_test %>% 
  mutate(error = abs(.pred - departures)) %>% 
  filter(departures > 5) %>% 
  select(year, month, day, hour, weekday, departures, .pred, error, conditions,temperature, cloud_cover, snow, precipitation, lincoln_memorial_departures)  %>% 
  arrange(error) %>% 
  head(15)
```

### Worst guessing with low departures - why?

```{r}
p_test %>% 
  mutate(error = abs(.pred - departures)) %>% 
  filter(departures < 5) %>% 
  select(year, month, day, hour, weekday, departures, .pred, error, conditions,temperature, cloud_cover, snow, precipitation, lincoln_memorial_departures)  %>% 
  arrange(-error) %>% 
  head(15)
```

### Worst guesses with low departures and actual weather

```{r}
p_test_no_wx <- p_test %>% 
    mutate(error = abs(.pred - departures)) %>% 
  select(year, month, day, hour, weekday, departures, .pred, error, lincoln_memorial_departures)
  
hour_data_wx <- hour_data %>% 
  select(year, month, day, hour, conditions,temperature, cloud_cover, precipitation, wind_speed, heat_index, wind_chill)

left_join(p_test_no_wx, hour_data_wx, on=c(year, month, day, hour)) %>% 
  filter(departures < 5) %>% 
  arrange(-error) %>% 
  head(15)

p_test %>% summarize(mean(wind_speed))
```

### Can we predict Lincoln Memorial will be empty? Every prediction for Sunday.

```{r}
p_test %>% 
  filter(weekday == "Sun") %>% 
  ggplot(aes(x = hour, y = .pred))+
  geom_point(color = "orange")
  # geom_smooth()
```

### After testing multiple variables, two week lagged data from two stations heavily weighted departures

```{r}
p_test %>% 
  filter(weekday == "Sun", hour == 14) %>% 
  select(`4th_st_&_madison_dr_nw_arrivals`, `4th_st_&_madison_dr_nw_departures`, `lincoln_memorial_departures`, lincoln_memorial_arrivals, .pred) %>% 
  pivot_longer(1:4, names_to = "station", values_to="value") %>% 
  ggplot(aes(x = value, y = .pred, color=station))+
  geom_point()
```

### Filtering out the high data point (Daylight Savings Time)

```{r}
# Only having one data point for certain times of the year hurts
# That one high datapoint is daylight savings time....
# only 23 hours in that day
p_test %>% 
  filter(weekday == "Sun", hour == 14) %>% 
  select(`4th_st_&_madison_dr_nw_arrivals`, `4th_st_&_madison_dr_nw_departures`, `lincoln_memorial_departures`, lincoln_memorial_arrivals, .pred) %>% 
  pivot_longer(1:4, names_to = "station", values_to="value") %>% 
  filter(value >= 1) %>% 
  ggplot(aes(x = value, y = .pred, color=station))+
  geom_smooth(aes(x = value, y = .pred, color=station), alpha=0.1)+
  geom_point()

# Look at Georgetown Harbor / 30th St NW
# 39th & Calvert St NW / Stoddert
```

Having two or more years would likely have helped that did not have a large change due to COVID.

## High Supply

```{r high_supply, cache=TRUE, message=FALSE, warning=FALSE}
cbs_data <- read_files('data/')

data <- sep_departures_from_arrivals(cbs_data) %>%
  filter(!is.na(station)) # Remove electric bikes not going to / from a station

filtered_data <- filter_by_distance(data, from_station = 'georgetown_harbor_/_30th_st_nw', distance_m = 1600)

hour_data <- get_station_hourly(filtered_data) %>%
  get_historic_weather() %>%
  add_sun_is_out()

lagged_data <- setup_for_time_prediction(hour_data, station_name = "georgetown_harbor_/_30th_st_nw")

model_data <- lagged_data %>%
  add_predictor_times() %>%
  filter(!is.na(departures) & !is.na(arrivals)) %>%
  select(-departures) %>% 
  format_weather()

# create a split object
data_split <- initial_split(model_data, prop = 0.8)

# create the training and testing data
train <- training(x = data_split)
test  <- testing(x = data_split)
```

```{r chosen_one_supply, cache=TRUE}
recipe <-
  recipe(formula = arrivals ~ ., data = train) %>%
  step_holiday(date, holidays = timeDate::listHolidays("US"), keep_original_cols = FALSE) %>% 
  step_nzv(all_predictors()) 
  # step_BoxCox(all_outcomes()) # Does not run with zero in the values

final_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(mod) %>%
  fit(train)

p_test <- bind_cols(
  test,
  predict(object = final_wf, new_data = test))
```

## High Supply Results

Out of sample RMSE

```{r OOS_RMSE_Supply}
sqrt(mean((p_test$arrivals - p_test$.pred)^2))
```

```{r supply_final_vip, warning=FALSE, eval=FALSE}
mod %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(arrivals ~ .,
    data = train
  ) %>%
  vip(geom = "point")
```

```{r high_supply_day}
p_test %>% 
  filter(month == "Aug" && year == 2021) %>% 
  ggplot(aes(x = hour, y = .pred))+
  geom_point(color = "orange")
  # geom_smooth()
```

## High Demand

```{r high_demand, cache=TRUE}
filtered_data <- filter_by_distance(data, from_station = '39th_&_calvert_st_nw_/_stoddert', distance_m = 1600)

hour_data <- get_station_hourly(filtered_data) %>%
  get_historic_weather() %>%
  add_sun_is_out()

lagged_data <- setup_for_time_prediction(hour_data, station_name = "39th_&_calvert_st_nw_/_stoddert")

model_data <- lagged_data %>%
  add_predictor_times() %>%
  filter(!is.na(departures) & !is.na(arrivals)) %>%
  select(-arrivals) %>% 
  format_weather()

# create a split object
data_split <- initial_split(model_data, prop = 0.8)

# create the training and testing data
train <- training(x = data_split)
test  <- testing(x = data_split)
```

```{r chosen_one_demand, cache=TRUE}
recipe <-
  recipe(formula = departures ~ ., data = train) %>%
  step_holiday(date, holidays = timeDate::listHolidays("US"), keep_original_cols = FALSE) %>% 
  step_nzv(all_predictors()) 
  # step_BoxCox(all_outcomes()) # Does not run with zero in the values

final_wf <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(mod) %>%
  fit(train)

p_test <- bind_cols(
  test,
  predict(object = final_wf, new_data = test))
```

## Results

Out of sample RMSE

```{r OOS_RMSE_Demand}
sqrt(mean((p_test$departures - p_test$.pred)^2))
```

```{r demand_final_vip, warning=FALSE, eval=FALSE}
mod %>%
  set_engine("ranger", importance = "permutation") %>%
  fit(departures ~ .,
    data = train
  ) %>%
  vip(geom = "point")
```

```{r high_demand_day}
p_test %>% 
  filter(month == "Aug" && year == 2021) %>% 
  ggplot(aes(x = hour, y = .pred))+
  geom_point(color = "orange")
  # geom_smooth()
```
 

# last thing 
## Ideas for future projects

* Try xgboost
* Account for the pandemic somehow (for example over the summer 2020 when there was a curfew)
* Try a version where we first implement a random forest model to predict a binary variable of whether bikes are departing or not. Then use the predictions to create a new variable and estimate a second random forest model that uses that variable as a predictor
* Instead of using RMSE, we could use a range of error that we envision for our project (eg. we can aim at always haviing 3 bikes at a station-- and hence our prediction would essentially be wether or not we have 3 bikes!)

